{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Feature Engineering & Preprocessing",
    "## HumanForYou — Employee Attrition Prediction",
    "",
    "---",
    "",
    "### Objective",
    "",
    "Transform raw merged data into **model-ready features**:",
    "1. Handle missing values with justified strategies",
    "2. Encode categorical variables (ordinal vs. one-hot)",
    "3. Engineer new features from existing ones",
    "4. Scale numerical features",
    "5. Address class imbalance (SMOTE)",
    "6. Export train/test splits for modeling",
    "",
    "> This notebook expects `merged_data.csv` from **01_Data_Validation_Pipeline**."
   ],
   "id": "4a41c888"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup"
   ],
   "id": "ef674950"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# IMPORTS\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "OUTPUT_DIR = \"../outputs\"\n",
    "df = pd.read_csv(f\"{OUTPUT_DIR}/merged_data.csv\")\n",
    "\n",
    "# Binary target\n",
    "df[\"Attrition\"] = (df[\"Attrition\"] == \"Yes\").astype(int)\n",
    "\n",
    "print(f\"Loaded: {df.shape[0]} rows × {df.shape[1]} columns\")"
   ],
   "outputs": [],
   "id": "4a505977"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Missing Value Treatment",
    "",
    "**Strategy**:",
    "- Survey columns (EnvironmentSatisfaction, JobSatisfaction, WorkLifeBalance): impute with **median** (ordinal data)",
    "- Badge features: impute with **median** (continuous data)",
    "- Justify: median is robust to outliers and preserves the ordinal nature of survey scales"
   ],
   "id": "ca26e532"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# MISSING VALUE IMPUTATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"MISSING VALUES BEFORE IMPUTATION\")\n",
    "print(\"=\" * 65)\n",
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "print(missing.to_string())\n",
    "\n",
    "# Survey columns — median imputation\n",
    "survey_cols = [\"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\"]\n",
    "for col in survey_cols:\n",
    "    if col in df.columns and df[col].isnull().any():\n",
    "        median_val = df[col].median()\n",
    "        df[col].fillna(median_val, inplace=True)\n",
    "        print(f\"  → {col}: imputed {df[col].isnull().sum()} NaN with median = {median_val}\")\n",
    "\n",
    "# Badge features — median imputation\n",
    "badge_cols = [\"avg_arrival_hour\", \"avg_departure_hour\", \"avg_working_hours\", \"absence_rate\", \"late_arrival_rate\"]\n",
    "for col in badge_cols:\n",
    "    if col in df.columns and df[col].isnull().any():\n",
    "        median_val = df[col].median()\n",
    "        df[col].fillna(median_val, inplace=True)\n",
    "        print(f\"  → {col}: imputed with median = {median_val:.3f}\")\n",
    "\n",
    "# NumCompaniesWorked — median imputation\n",
    "if \"NumCompaniesWorked\" in df.columns and df[\"NumCompaniesWorked\"].isnull().any():\n",
    "    df[\"NumCompaniesWorked\"].fillna(df[\"NumCompaniesWorked\"].median(), inplace=True)\n",
    "\n",
    "# TotalWorkingYears\n",
    "if \"TotalWorkingYears\" in df.columns and df[\"TotalWorkingYears\"].isnull().any():\n",
    "    df[\"TotalWorkingYears\"].fillna(df[\"TotalWorkingYears\"].median(), inplace=True)\n",
    "\n",
    "print(f\"\\nRemaining missing values: {df.isnull().sum().sum()}\")"
   ],
   "outputs": [],
   "id": "60770412"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Feature Engineering",
    "",
    "Create new meaningful features from existing data to improve model performance."
   ],
   "id": "bf719e6c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# NEW FEATURES\n",
    "# ==============================================================================\n",
    "\n",
    "# Income per job level — normalizes salary by hierarchy\n",
    "if \"MonthlyIncome\" in df.columns and \"JobLevel\" in df.columns:\n",
    "    df[\"IncomePerJobLevel\"] = df[\"MonthlyIncome\"] / df[\"JobLevel\"]\n",
    "\n",
    "# Years since last promotion relative to company tenure\n",
    "if \"YearsSinceLastPromotion\" in df.columns and \"YearsAtCompany\" in df.columns:\n",
    "    df[\"PromotionStagnation\"] = df[\"YearsSinceLastPromotion\"] / (df[\"YearsAtCompany\"] + 1)\n",
    "\n",
    "# Satisfaction composite score (mean of survey items)\n",
    "survey_items = [\"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\"]\n",
    "existing_items = [c for c in survey_items if c in df.columns]\n",
    "if existing_items:\n",
    "    df[\"SatisfactionScore\"] = df[existing_items].mean(axis=1)\n",
    "\n",
    "# Years with manager / years at company ratio\n",
    "if \"YearsWithCurrManager\" in df.columns and \"YearsAtCompany\" in df.columns:\n",
    "    df[\"ManagerStability\"] = df[\"YearsWithCurrManager\"] / (df[\"YearsAtCompany\"] + 1)\n",
    "\n",
    "# Overtime proxy: working hours > 9h\n",
    "if \"avg_working_hours\" in df.columns:\n",
    "    df[\"LongHours\"] = (df[\"avg_working_hours\"] > 9).astype(int)\n",
    "\n",
    "new_features = [\"IncomePerJobLevel\", \"PromotionStagnation\", \"SatisfactionScore\", \"ManagerStability\", \"LongHours\"]\n",
    "new_features = [f for f in new_features if f in df.columns]\n",
    "print(f\"New features created: {new_features}\")\n",
    "df[new_features].describe().round(3)"
   ],
   "outputs": [],
   "id": "47243ef7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Categorical Encoding",
    "",
    "**Strategy**:",
    "- **Ordinal encoding** for features with natural order (Education, BusinessTravel)",
    "- **One-hot encoding** for nominal features (Department, EducationField, JobRole, Gender, MaritalStatus)",
    "",
    "> **Ethical note**: Gender and MaritalStatus are encoded but will be monitored for fairness during model evaluation."
   ],
   "id": "7756702d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# CATEGORICAL ENCODING\n",
    "# ==============================================================================\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "print(f\"Categorical columns to encode: {cat_cols}\")\n",
    "\n",
    "# Ordinal encoding for BusinessTravel\n",
    "bt_map = {\"Non-Travel\": 0, \"Travel_Rarely\": 1, \"Travel_Frequently\": 2}\n",
    "if \"BusinessTravel\" in df.columns:\n",
    "    df[\"BusinessTravel\"] = df[\"BusinessTravel\"].map(bt_map)\n",
    "\n",
    "# One-hot encoding for remaining categoricals\n",
    "ohe_cols = [c for c in cat_cols if c != \"BusinessTravel\"]\n",
    "df = pd.get_dummies(df, columns=ohe_cols, drop_first=True, dtype=int)\n",
    "\n",
    "print(f\"\\nPost-encoding shape: {df.shape[0]} rows × {df.shape[1]} columns\")"
   ],
   "outputs": [],
   "id": "91c646e0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Train / Test Split & Scaling"
   ],
   "id": "b665dbdd"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# TRAIN / TEST SPLIT\n",
    "# ==============================================================================\n",
    "\n",
    "# Drop EmployeeID (not a feature)\n",
    "if \"EmployeeID\" in df.columns:\n",
    "    employee_ids = df[\"EmployeeID\"].copy()  # save for later traceability\n",
    "    df.drop(columns=[\"EmployeeID\"], inplace=True)\n",
    "\n",
    "X = df.drop(columns=[\"Attrition\"])\n",
    "y = df[\"Attrition\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({y_train.mean()*100:.1f}% attrition)\")\n",
    "print(f\"Test set:  {X_test.shape[0]} samples ({y_test.mean()*100:.1f}% attrition)\")"
   ],
   "outputs": [],
   "id": "68fb106e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# FEATURE SCALING — StandardScaler\n",
    "# ==============================================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled  = pd.DataFrame(scaler.transform(X_test),      columns=X_test.columns,  index=X_test.index)\n",
    "\n",
    "print(\"Scaling applied (StandardScaler — fit on train only).\")"
   ],
   "outputs": [],
   "id": "6e876435"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Class Imbalance — SMOTE",
    "",
    "Apply SMOTE **only on the training set** to avoid data leakage."
   ],
   "id": "5f12d22f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# SMOTE OVERSAMPLING (train set only)\n",
    "# ==============================================================================\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Before SMOTE: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"After  SMOTE: {pd.Series(y_train_resampled).value_counts().to_dict()}\")"
   ],
   "outputs": [],
   "id": "3dff9ef9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Export Preprocessed Data"
   ],
   "id": "77738aa7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# EXPORT\n",
    "# ==============================================================================\n",
    "import joblib\n",
    "\n",
    "# Save processed data\n",
    "X_train_resampled.to_csv(f\"{OUTPUT_DIR}/X_train.csv\", index=False)\n",
    "X_test_scaled.to_csv(f\"{OUTPUT_DIR}/X_test.csv\", index=False)\n",
    "pd.Series(y_train_resampled, name=\"Attrition\").to_csv(f\"{OUTPUT_DIR}/y_train.csv\", index=False)\n",
    "y_test.to_csv(f\"{OUTPUT_DIR}/y_test.csv\", index=False)\n",
    "\n",
    "# Also save non-SMOTE versions for fairness analysis\n",
    "X_train_scaled.to_csv(f\"{OUTPUT_DIR}/X_train_no_smote.csv\", index=False)\n",
    "y_train.to_csv(f\"{OUTPUT_DIR}/y_train_no_smote.csv\", index=False)\n",
    "\n",
    "# Save scaler for reproducibility\n",
    "joblib.dump(scaler, f\"{OUTPUT_DIR}/scaler.joblib\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names = list(X_train.columns)\n",
    "pd.Series(feature_names).to_csv(f\"{OUTPUT_DIR}/feature_names.csv\", index=False, header=False)\n",
    "\n",
    "print(f\"Exported to {OUTPUT_DIR}/:\")\n",
    "print(f\"  X_train.csv          ({X_train_resampled.shape})\")\n",
    "print(f\"  X_test.csv           ({X_test_scaled.shape})\")\n",
    "print(f\"  y_train.csv          ({len(y_train_resampled)})\")\n",
    "print(f\"  y_test.csv           ({len(y_test)})\")\n",
    "print(f\"  feature_names.csv    ({len(feature_names)} features)\")\n",
    "print(f\"  scaler.joblib\")\n",
    "print(\"\\n✓ Preprocessing complete — proceed to 04_Model_Benchmark.ipynb\")"
   ],
   "outputs": [],
   "id": "9332f4ad"
  }
 ]
}