{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Data Validation & Merge Pipeline",
    "## HumanForYou — Employee Attrition Prediction",
    "",
    "---",
    "",
    "### Objective",
    "",
    "Establish a **reproducible and traceable workflow** to:",
    "1. Validate the integrity of all 4 source datasets (general_data, employee_survey, manager_survey, badge data)",
    "2. Check schema consistency, missing values, and data quality",
    "3. Merge into a single analysis-ready DataFrame",
    "4. Export the clean dataset for downstream notebooks",
    "",
    "> **Adapted from** the pipeline architecture of a previous YOLO detection project — same rigor, different domain."
   ],
   "id": "6e87b8a6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Configuration and Environment Setup"
   ],
   "id": "67f33ead"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURATION — Centralized paths and parameters\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Path Configuration ---\n",
    "# Adapt DATA_DIR to your environment\n",
    "DATA_DIR = \"../data\"          # where CSVs live\n",
    "OUTPUT_DIR = \"../outputs\"     # cleaned / merged outputs\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Source files declaration\n",
    "FILES = {\n",
    "    \"general\":         os.path.join(DATA_DIR, \"general_data.csv\"),\n",
    "    \"employee_survey\": os.path.join(DATA_DIR, \"employee_survey_data.csv\"),\n",
    "    \"manager_survey\":  os.path.join(DATA_DIR, \"manager_survey_data.csv\"),\n",
    "    \"in_time\":         os.path.join(DATA_DIR, \"in_time.csv\"),\n",
    "    \"out_time\":        os.path.join(DATA_DIR, \"out_time.csv\"),\n",
    "}\n",
    "\n",
    "EMPLOYEE_ID_COL = \"EmployeeID\"\n",
    "\n",
    "print(f\"Configuration loaded — {datetime.now():%Y-%m-%d %H:%M}\")\n",
    "print(f\"Data directory : {os.path.abspath(DATA_DIR)}\")\n",
    "print(f\"Output directory: {os.path.abspath(OUTPUT_DIR)}\")"
   ],
   "outputs": [],
   "id": "4c2d710e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: File Existence & Schema Validation",
    "",
    "**Purpose**: Verify every source file exists, is non-empty, and has the expected columns."
   ],
   "id": "01a06b14"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# FILE EXISTENCE CHECK\n",
    "# ==============================================================================\n",
    "\n",
    "def validate_file(name, path):\n",
    "    \"\"\"Check file exists and is non-empty. Return row/col counts.\"\"\"\n",
    "    if not os.path.isfile(path):\n",
    "        print(f\"  [FAIL] {name}: file not found at {path}\")\n",
    "        return None\n",
    "    size_kb = os.path.getsize(path) / 1024\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"  [OK]   {name:20s} — {df.shape[0]:>5} rows × {df.shape[1]:>3} cols  ({size_kb:,.0f} KB)\")\n",
    "    return df\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"FILE VALIDATION\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "raw = {}\n",
    "for key, path in FILES.items():\n",
    "    result = validate_file(key, path)\n",
    "    if result is not None:\n",
    "        raw[key] = result\n",
    "\n",
    "print(f\"\\nLoaded {len(raw)}/{len(FILES)} files successfully.\")"
   ],
   "outputs": [],
   "id": "4fe38d00"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# SCHEMA VALIDATION — Expected columns\n",
    "# ==============================================================================\n",
    "\n",
    "EXPECTED_SCHEMAS = {\n",
    "    \"general\": [\n",
    "        \"Age\", \"Attrition\", \"BusinessTravel\", \"Department\", \"DistanceFromHome\",\n",
    "        \"Education\", \"EducationField\", \"EmployeeCount\", \"EmployeeID\", \"Gender\",\n",
    "        \"JobLevel\", \"JobRole\", \"MaritalStatus\", \"MonthlyIncome\",\n",
    "        \"NumCompaniesWorked\", \"Over18\", \"PercentSalaryHike\", \"StandardHours\",\n",
    "        \"StockOptionLevel\", \"TotalWorkingYears\", \"TrainingTimesLastYear\",\n",
    "        \"YearsAtCompany\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\"\n",
    "    ],\n",
    "    \"employee_survey\": [\"EmployeeID\", \"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\"],\n",
    "    \"manager_survey\":  [\"EmployeeID\", \"JobInvolvement\", \"PerformanceRating\"],\n",
    "}\n",
    "\n",
    "print(\"SCHEMA VALIDATION\")\n",
    "print(\"=\" * 65)\n",
    "for key, expected_cols in EXPECTED_SCHEMAS.items():\n",
    "    actual = list(raw[key].columns)\n",
    "    missing = set(expected_cols) - set(actual)\n",
    "    extra   = set(actual) - set(expected_cols)\n",
    "    status = \"OK\" if not missing else \"FAIL\"\n",
    "    print(f\"  [{status}] {key}\")\n",
    "    if missing:\n",
    "        print(f\"         Missing columns: {missing}\")\n",
    "    if extra:\n",
    "        print(f\"         Extra columns  : {extra}\")"
   ],
   "outputs": [],
   "id": "557c8e7d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Data Quality Audit",
    "",
    "**Purpose**: For each dataset, check missing values, duplicates, constant columns, and basic statistics."
   ],
   "id": "c484068e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# MISSING VALUES REPORT\n",
    "# ==============================================================================\n",
    "\n",
    "def missing_report(df, name):\n",
    "    \"\"\"Print missing value summary for a DataFrame.\"\"\"\n",
    "    total = df.isnull().sum()\n",
    "    pct   = (total / len(df) * 100).round(2)\n",
    "    report = pd.DataFrame({\"missing\": total, \"pct\": pct})\n",
    "    report = report[report[\"missing\"] > 0].sort_values(\"pct\", ascending=False)\n",
    "    if report.empty:\n",
    "        print(f\"  {name}: No missing values ✓\")\n",
    "    else:\n",
    "        print(f\"  {name}: {len(report)} column(s) with missing values\")\n",
    "        for col, row in report.iterrows():\n",
    "            print(f\"    → {col}: {int(row['missing'])} ({row['pct']:.1f}%)\")\n",
    "    return report\n",
    "\n",
    "print(\"MISSING VALUES AUDIT\")\n",
    "print(\"=\" * 65)\n",
    "missing_reports = {}\n",
    "for key, df in raw.items():\n",
    "    if key not in (\"in_time\", \"out_time\"):  # badge data handled separately\n",
    "        missing_reports[key] = missing_report(df, key)"
   ],
   "outputs": [],
   "id": "6b80f363"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# DUPLICATE & CONSTANT COLUMN CHECK\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"DUPLICATE EMPLOYEES CHECK\")\n",
    "print(\"=\" * 65)\n",
    "for key in [\"general\", \"employee_survey\", \"manager_survey\"]:\n",
    "    df = raw[key]\n",
    "    n_dup = df[EMPLOYEE_ID_COL].duplicated().sum()\n",
    "    print(f\"  {key}: {n_dup} duplicate EmployeeIDs {'✓' if n_dup == 0 else '⚠'}\")\n",
    "\n",
    "print(\"\\nCONSTANT COLUMNS CHECK\")\n",
    "print(\"=\" * 65)\n",
    "df_gen = raw[\"general\"]\n",
    "constant_cols = [col for col in df_gen.columns if df_gen[col].nunique() <= 1]\n",
    "if constant_cols:\n",
    "    print(f\"  Constant columns found (candidates for removal): {constant_cols}\")\n",
    "    for col in constant_cols:\n",
    "        print(f\"    → {col}: unique value = {df_gen[col].unique()}\")\n",
    "else:\n",
    "    print(\"  No constant columns found.\")"
   ],
   "outputs": [],
   "id": "8769c0a5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# EMPLOYEE ID CONSISTENCY ACROSS DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"EMPLOYEE ID CROSS-FILE CONSISTENCY\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "ids_general  = set(raw[\"general\"][EMPLOYEE_ID_COL])\n",
    "ids_employee = set(raw[\"employee_survey\"][EMPLOYEE_ID_COL])\n",
    "ids_manager  = set(raw[\"manager_survey\"][EMPLOYEE_ID_COL])\n",
    "\n",
    "# in_time / out_time: first column is EmployeeID (unnamed)\n",
    "ids_in  = set(raw[\"in_time\"].iloc[:, 0].astype(int))\n",
    "ids_out = set(raw[\"out_time\"].iloc[:, 0].astype(int))\n",
    "\n",
    "all_sets = {\n",
    "    \"general\": ids_general, \"employee_survey\": ids_employee,\n",
    "    \"manager_survey\": ids_manager, \"in_time\": ids_in, \"out_time\": ids_out\n",
    "}\n",
    "\n",
    "ref = ids_general\n",
    "for name, s in all_sets.items():\n",
    "    only_ref  = ref - s\n",
    "    only_this = s - ref\n",
    "    if not only_ref and not only_this:\n",
    "        print(f\"  {name:20s} ↔ general: Perfect match ({len(s)} IDs) ✓\")\n",
    "    else:\n",
    "        print(f\"  {name:20s} ↔ general: {len(only_ref)} missing, {len(only_this)} extra ⚠\")"
   ],
   "outputs": [],
   "id": "922124b0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Badge Data Processing",
    "",
    "**Purpose**: Transform raw badge timestamps (in_time / out_time) into meaningful features per employee:",
    "- Average arrival & departure hours",
    "- Average daily working hours",
    "- Absence rate (% of working days with no badge)",
    "- Punctuality indicators"
   ],
   "id": "5e9561b5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# BADGE DATA — Parse and compute features\n",
    "# ==============================================================================\n",
    "\n",
    "def process_badge_data(df_in, df_out):\n",
    "    \"\"\"\n",
    "    Transform raw badge in/out timestamps into employee-level features.\n",
    "    \n",
    "    Returns a DataFrame indexed by EmployeeID with:\n",
    "      - avg_arrival_hour, avg_departure_hour, avg_working_hours\n",
    "      - absence_rate (fraction of working days with NA badge)\n",
    "      - late_arrival_rate (fraction of days arriving after 10:00)\n",
    "    \"\"\"\n",
    "    # Extract EmployeeID from first column\n",
    "    emp_ids = df_in.iloc[:, 0].astype(int)\n",
    "    \n",
    "    # Date columns (skip first = EmployeeID)\n",
    "    date_cols = df_in.columns[1:]\n",
    "    \n",
    "    records = []\n",
    "    for idx in range(len(emp_ids)):\n",
    "        emp_id = emp_ids.iloc[idx]\n",
    "        \n",
    "        arrivals = []\n",
    "        departures = []\n",
    "        n_absent = 0\n",
    "        n_late = 0\n",
    "        n_days = len(date_cols)\n",
    "        \n",
    "        for col in date_cols:\n",
    "            in_val  = df_in.iloc[idx][col]\n",
    "            out_val = df_out.iloc[idx][col]\n",
    "            \n",
    "            # Check for absence (NA or NaN)\n",
    "            in_missing  = pd.isna(in_val) or (isinstance(in_val, str) and in_val.strip().upper() == \"NA\")\n",
    "            out_missing = pd.isna(out_val) or (isinstance(out_val, str) and out_val.strip().upper() == \"NA\")\n",
    "            \n",
    "            if in_missing or out_missing:\n",
    "                n_absent += 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                t_in  = pd.to_datetime(in_val)\n",
    "                t_out = pd.to_datetime(out_val)\n",
    "                arrivals.append(t_in.hour + t_in.minute / 60)\n",
    "                departures.append(t_out.hour + t_out.minute / 60)\n",
    "                if t_in.hour >= 10:\n",
    "                    n_late += 1\n",
    "            except Exception:\n",
    "                n_absent += 1\n",
    "        \n",
    "        n_present = len(arrivals)\n",
    "        records.append({\n",
    "            EMPLOYEE_ID_COL: emp_id,\n",
    "            \"avg_arrival_hour\":   np.mean(arrivals)    if arrivals else np.nan,\n",
    "            \"avg_departure_hour\": np.mean(departures)  if departures else np.nan,\n",
    "            \"avg_working_hours\":  np.mean([d - a for a, d in zip(arrivals, departures)]) if arrivals else np.nan,\n",
    "            \"absence_rate\":       n_absent / n_days    if n_days > 0 else np.nan,\n",
    "            \"late_arrival_rate\":  n_late / n_present   if n_present > 0 else np.nan,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "print(\"Processing badge data (this may take ~30s)...\")\n",
    "df_badge = process_badge_data(raw[\"in_time\"], raw[\"out_time\"])\n",
    "print(f\"Badge features computed for {len(df_badge)} employees.\")\n",
    "df_badge.describe().round(3)"
   ],
   "outputs": [],
   "id": "45a6a45f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Dataset Merge",
    "",
    "**Purpose**: Inner-join all datasets on EmployeeID to create the unified analysis DataFrame."
   ],
   "id": "c88b2434"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# MERGE ALL DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "df = raw[\"general\"].copy()\n",
    "\n",
    "# Merge survey data\n",
    "df = df.merge(raw[\"employee_survey\"], on=EMPLOYEE_ID_COL, how=\"left\")\n",
    "df = df.merge(raw[\"manager_survey\"],  on=EMPLOYEE_ID_COL, how=\"left\")\n",
    "df = df.merge(df_badge,               on=EMPLOYEE_ID_COL, how=\"left\")\n",
    "\n",
    "# Drop constant / uninformative columns identified earlier\n",
    "cols_to_drop = [\"EmployeeCount\", \"Over18\", \"StandardHours\"]\n",
    "df.drop(columns=[c for c in cols_to_drop if c in df.columns], inplace=True)\n",
    "\n",
    "print(f\"Merged dataset: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"Target distribution (Attrition):\")\n",
    "print(df[\"Attrition\"].value_counts())\n",
    "print(f\"\\nAttrition rate: {(df['Attrition'] == 'Yes').mean() * 100:.1f}%\")"
   ],
   "outputs": [],
   "id": "0417cb16"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# FINAL QUALITY CHECK ON MERGED DATA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"POST-MERGE QUALITY CHECK\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Missing values in merged dataset\n",
    "total_missing = df.isnull().sum()\n",
    "cols_with_na = total_missing[total_missing > 0]\n",
    "if cols_with_na.empty:\n",
    "    print(\"  No missing values in merged dataset ✓\")\n",
    "else:\n",
    "    print(f\"  {len(cols_with_na)} columns with missing values:\")\n",
    "    for col, count in cols_with_na.items():\n",
    "        print(f\"    → {col}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Data types summary\n",
    "print(f\"\\nColumn types:\")\n",
    "print(f\"  Numeric : {df.select_dtypes(include='number').shape[1]}\")\n",
    "print(f\"  Object  : {df.select_dtypes(include='object').shape[1]}\")\n",
    "print(f\"  Total   : {df.shape[1]}\")"
   ],
   "outputs": [],
   "id": "0a0f6199"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Export"
   ],
   "id": "ce8bac84"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# EXPORT CLEAN DATASET\n",
    "# ==============================================================================\n",
    "\n",
    "output_path = os.path.join(OUTPUT_DIR, \"merged_data.csv\")\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Merged dataset exported to: {os.path.abspath(output_path)}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# Also save a quick summary\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"data_validation_summary.txt\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(f\"Data Validation Summary — {datetime.now():%Y-%m-%d %H:%M}\\n\")\n",
    "    f.write(f\"{'='*50}\\n\")\n",
    "    f.write(f\"Total employees: {len(df)}\\n\")\n",
    "    f.write(f\"Total features:  {df.shape[1]}\\n\")\n",
    "    f.write(f\"Attrition rate:  {(df['Attrition']=='Yes').mean()*100:.1f}%\\n\")\n",
    "    f.write(f\"Columns dropped: {cols_to_drop}\\n\")\n",
    "    f.write(f\"Badge features added: avg_arrival_hour, avg_departure_hour, avg_working_hours, absence_rate, late_arrival_rate\\n\")\n",
    "\n",
    "print(f\"Summary saved to: {os.path.abspath(summary_path)}\")\n",
    "print(\"\\n✓ Pipeline complete — proceed to 02_EDA_Explorer.ipynb\")"
   ],
   "outputs": [],
   "id": "6b59c72b"
  }
 ]
}